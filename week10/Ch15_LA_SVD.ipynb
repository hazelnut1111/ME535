{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "#notebook-container {\n",
       "    width: 100%\n",
       "}\n",
       "\n",
       ".code_cell {\n",
       "   flex-direction: row !important;\n",
       "}\n",
       "\n",
       ".code_cell .input {\n",
       "    width: 50%\n",
       "}\n",
       "\n",
       ".code_cell .output_wrapper {\n",
       "    width: 50%\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg as LA\n",
    "from simple_plot import T\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    "#notebook-container {\n",
    "    width: 100%\n",
    "}\n",
    "\n",
    ".code_cell {\n",
    "   flex-direction: row !important;\n",
    "}\n",
    "\n",
    ".code_cell .input {\n",
    "    width: 50%\n",
    "}\n",
    "\n",
    ".code_cell .output_wrapper {\n",
    "    width: 50%\n",
    "}\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ch. 15 - Linear Algebra and Singular Value Decomposition\n",
    "\n",
    "We have encountered linear algebra on several occasions starting with the intrinsic case of solving systems of linear equations, then in the context of curve fitting, and most recently in the context of linear programming problems in optimization. Here we aim to generalize (and take a more modern perspective on) some of our linear algebra methods to develop tools that are especially useful in an era of data-driven analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\text{\\S} 15.1$ Basics of the Singular Value Decomposition (SVD)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of important linear algebra methods that we have encountered can be both better understood and more compactly written as matrix decomposition/factorization:\n",
    "\n",
    "- Gaussian elimination translated to $LU$ factorization\n",
    "    - $Ax=b \\rightarrow LUx=b$ (where $L$ and $U$ are lower/upper triangular respectively)\n",
    "    - Reduce solution of general linear system to a pair of triangular problems: \n",
    "        - $Ux = y$\n",
    "        - $Ly = b$\n",
    "- The search for an orthogonal basis (via Gram-Schmidt, Householder, etc.) led to $QR$ decomposition\n",
    "    - Orthogonal $Q$\n",
    "    - \"Rectangular\" $R$\n",
    "- Eigenvalue problems lead to eigendecomposition $A = S \\Lambda S^{-1}$\n",
    "    - Approach to obtaining the eigendecomposition:\n",
    "        - Each eigenvalue/vector pair satisfies $A x_i = \\lambda_i x_i$\n",
    "        - Collect the eigenvectors as columns of matrix $S$\n",
    "        - Form $\\Lambda$ as diagonal matrix of corresponding eigenvalues \n",
    "        - $\\rightarrow AS = S \\Lambda$\n",
    "        - Right multiply by $S^{-1} \\implies A = S \\Lambda S^{-1}$\n",
    "    - Extremely useful for real, symmetric matrices\n",
    "        - Arise frequently in applications (e.g. dynamics and vibrations)\n",
    "        - Have real eigenvalues\n",
    "        - Eigenvectors for distinct eigenvalues are orthogonal\n",
    "        - All eigenvalues distinct $\\implies$:\n",
    "            - $\\exists$ full set of orthogonal eigenvectors\n",
    "            - Eigenvectors form orthogonal basis\n",
    "            - Normalized eigenvectors as columns form an orthogonal matrix Q\n",
    "            - $Q^{-1} = Q^T$ so the inverse does not require computation\n",
    "            - $A = S \\Lambda S^{-1} \\rightarrow A = Q \\Lambda Q^T$\n",
    "    - __Eigendecomposition does not exist for all matrices.__ Fails for:\n",
    "        - Rectangular A\n",
    "        - $S$ non-invertible (which is the general case when $A^T \\neq A$)\n",
    "\n",
    "> __Note on real vs. complex data:__ We will keep the notation simpler and more familiar by assuming that the data (the elements of $A$) are real, so we will refer to orthogonal matrices for which $Q^T = Q^{-1}$. If you end up dealing with the complex case, remember to replace \"orthogonal matrix\" with \"unitary matrix\" and change \"transpose\" to \"adjoint\" (or \"tranjugate\" which mashes up transpose and conjugate); i.e. $\\left(Q^* = \\overline{Q}^T = Q^{-1}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Singular Value Decomposition (SVD) provides the generalization of eigendecomposition that works for all matrices.__\n",
    "\n",
    "Where does that come from?\n",
    "\n",
    "To be concrete, let's consider matrix multiplication, $Ax=y$, with an $m \\times n$ matrix $A$ (i.e. A has $m$ rows and $n$ columns). \n",
    "- Conformability requires the length of $x$ to match up with $n$ (the number of columns of $A$)\n",
    "- The length of the output $y$ matches up with $m$ (the number of rows of $A$)\n",
    "- Multiplication by $m \\times n$ matrix $A$ takes an input $x$ with length $n$ and produces output $y$ with length $m$\n",
    "- $A : \\R^n \\rightarrow \\R^m$\n",
    "\n",
    "When we previously considered square matrices, matrix multiplication mapped vectors into the same space (or at least an equivalent space of the same dimension), and we could think of individual vectors being stretched and rotated as a result of multiplication by $A$.\n",
    "\n",
    "For the general rectangular, it does not make sense to think of the output vector as being a rotated and stretched version of the input vector; they do not even belong to the same space so there is no good way to do the comparison of length/direction.\n",
    "\n",
    "Instead we need to think about relating items of the dimension of each space that we can compare. A geometric image of this concept is to consider our input in $\\R^N$ as an $n$-dimensional hypersphere (e.g. the collection of all unit-length vectors in $\\R^n$).\n",
    "\n",
    "Performing the operation \"multiply by $A$\" on the hypersphere means multiply all of the unit vectors in $\\R^n$ by $A$. Matrix multiplication on each input vector produces a vector in $\\R^m$ and we want to characterize the collection of output vectors.\n",
    "\n",
    "The geometric \"bottom line\" is that multiplication by $A_{m \\times n}$ transforms a hypersphere in $\\R^n$ into a hyperellipse in $R^m$ that can be characterized by the principle semi-axis lengths $\\sigma_1, \\sigma_2, \\ldots, \\sigma_m$ associated with the principal directional unit vectors $u_1, u_2, \\ldots, u_m$ that arose from multiplication of a set of vectors $v_1, v_2, \\ldots, v_n \\in \\R^n$ by the matrix $A$.\n",
    "\n",
    "> Note that the geometric dimension of the hyperellipse will match up with $r = rank(A)$ which can be less than $m$. This implies that some of the singular values can be zero. However, we do not need to consider $\\sigma < 0$. Why not? We are considering an entire hypersphere as input, so we could take the diametrically opposed input vector to undo any sign changes that might arise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea that a particular input unit vector gives rise to a particular principal semi-axis of the ouput hyperellipse  can be written as:\n",
    "$$A v_j = \\sigma_j u_j, \\quad 1 \\leq j \\leq n$$\n",
    "where the $v_j$ vectors provide an orthogonal basis for $\\R^n$ and the $u_j$ vectors associated with the principal directions of the hyperellipse provide an ortogonal basis for some sub-space of $\\R^m$ of dimension $d = r \\leq n$.\n",
    "\n",
    "Consistent with our idea of considering the collections of inputs and outputs, we group these individual relations to obtain the set of relations:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\\\\n",
    "\\\\\n",
    "A \\\\\n",
    "\\\\\n",
    "\\\\\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "\\\\\n",
    "v_1 & v_2 & \\ldots & v_n \\\\\n",
    "\\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\\\\n",
    "u_1 & u_2 & \\ldots & u_n \\\\\n",
    "\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\sigma_1 & & & \\\\\n",
    "& \\sigma_2 & & \\\\\n",
    "&  & \\ddots &  \\\\\n",
    " &  & &\\sigma_n \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "that can be written in matrix notation as\n",
    "$$A V = \\hat{U} \\hat{\\Sigma}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a typical case where $A$ is a tall, skinny matrix (with more rows than columns, so $m>n$). The matrices in the equation above are as follows:\n",
    "- A is the $m \\times n$ matrix being analyzed.\n",
    "- $\\hat{\\Sigma}$ is an $n \\times n$ diagonal matrix with the number of positive entries corresponding to $rank(A)$ (so with positive diagonal entries if $A$ is of full rank).\n",
    "- $\\hat{U}$ is a tall, skinny ($m \\times n$) matrix with orthonormal columns.\n",
    "- $\\hat{V}$ is an $n \\times n$ orthogonal/unitary matrix.\n",
    "\n",
    "Because $V$ is orthogonal, $V^{-1} = V^T$ and we can solve for $A$ (and complete the first version of the decomposition) by right-muliplying by $V^T$ to get the __Reduced (or \"Economy\") SVD Decomposition__:\n",
    "\n",
    "$$A = \\hat{U} \\hat{\\Sigma} V^T$$\n",
    "\n",
    "The shapes of the relevant matrices are illustrated in Fig. 15.3 below from the text by Kutz. (Remember that we are focusing on real matrices, so you can read $V^*$ as $V^T$ and \"unitary\" as \"orthogonal\" until you encounter complex matrices.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![<img src=\"kutzSVD_Figs_15.3_15.4.png\" width=\"200\"/>](kutzSVD_Figs_15.3_15.4.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is seen more frequently in the literature is the \"full\" SVD decomposition where $U$ is augmented with $m-n$ additional columns to become an $m \\times m$ square unitary matrix and $\\Sigma$ is augmented with $m-n$ rows of zeros as depicted in Fig. 15.4 above (again from the text by Kutz).\n",
    "\n",
    "Here is a description of the matrices in the full SVD:\n",
    "- $U$ is $m \\times m$ orthogonal/unitary\n",
    "- $V$ is $n \\times n$ orthogonal/unitary\n",
    "- $\\Sigma$ is $m \\times n$ diagonal\n",
    "- Diagonal entries of $\\Sigma$ are non-negative and ordered from largest to smallest: \n",
    "    - $\\sigma_1 \\geq \\sigma_2 \\geq \\ldots \\geq \\sigma_p \\geq 0$ where $p = min(m,n)$\n",
    "\n",
    "All of this sets up a theorem assuring the existence of the SVD:\n",
    "\n",
    "__Theorem__: Every matrix $A \\in \\Complex^{m \\times n}$ has a singular value decompostion $A = U \\Sigma V^*$. The singular values $\\{\\sigma_j \\}$ are uniquely determined and the singular vectors $\\{u_j \\}$ and $\\{v_j \\}$ are uniquely determined up to complex scale factors of unit magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying the SVD\n",
    "\n",
    "Now that we know the SVD applies to all matrices, let's consider how to compute it.\n",
    "\n",
    "An initial approach involves the normal matrix that we obtain by multiplying by $A^T$ on either the right or left:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "A^T A &= \\left(U \\Sigma V^T\\right)^T \\left(U \\Sigma V^T\\right) \\\\\n",
    "&= \\left(U \\Sigma V^T\\right)^T \\left(U \\Sigma V^T\\right) \\\\\n",
    "&= V \\Sigma U^T U \\Sigma V^T \\\\\n",
    "&= V \\Sigma^2 V^T\n",
    "\\end{aligned}$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "A A^T &= \\left(U \\Sigma V^T\\right) \\left(U \\Sigma V^T\\right)^T \\\\\n",
    "&= \\left(U \\Sigma V^T\\right) \\left(U \\Sigma V^T\\right)^T \\\\\n",
    "&= U \\Sigma V^T V \\Sigma U^T \\\\\n",
    "&= U \\Sigma^2 U^T\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying on the right by $V$ and $U$ gives\n",
    "\n",
    "$$\\begin{aligned}\n",
    "A^T A V = V \\Sigma^2 \\\\\n",
    "A A^T U = U \\Sigma^2\n",
    "\\end{aligned}$$\n",
    "\n",
    "Note that we have now recovered versions of the eigenproblem $MV = V \\Lambda$ which describes how a square matrix $M$ operates to stretch (and possibly reverse the direction of) eigenvectors $v_j$ to produce $\\lambda_j v_j$.\n",
    "\n",
    "We can now conclude that:\n",
    "- The left singular vectors $\\{u_j\\}$ of the SVD correspond to the eigenvectors of $A A^T$\n",
    "- The right singular vectors $\\{v_j\\}$ of the SVD correspond to the eigenvectors of $A^T A$\n",
    "- The singular values of $\\{ \\sigma_j \\}$ are the positive eigenvalues of $A A^T$ (or $A^T A$)\n",
    "\n",
    "Recall from our discussion of normal matrices in the context of least-squares fitting, actually doing the matrix multiplication to produce a normal matrix increases the condition number and has a detrimental effect on numerical accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> More appropriate methods for computing the SVD avoid dealing with the normal matrix (and its bad effect on condition number)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An alternative view of the SVD\n",
    "\n",
    "__Pieces of the SVD ($\\grave{a} \\text{ } la$ Strang)__ (\"Linear Algebra and Learning from Data,\" Wellesley, 2019)\n",
    "$$A = \\sigma_1 u_1 v_1^T + \\sigma_2 u_2 v_2^T + \\sigma_3 u_3 v_3^T + \\ldots + \\sigma_r u_r v_r^T$$\n",
    "\n",
    "- Each term involves a scalar multiple (specifically a singular value) of a product of:\n",
    "    - a basis vectors from $\\R^m$ ($u_i$ with dimensions $m \\times 1$)\n",
    "    - the transpose of a basis vector from $\\R^n$ ($v_j^T$ with dimensions $1 \\times n$) \n",
    "    - produces an $m \\times n$ matrix (i.e., the same dimensions as the original matrix $A$).\n",
    "    - specifies a __rank 1 matrix__ because it is the outer product of 1 vector from each of the relevant spaces.\n",
    "\n",
    "The \"pieces of the SVD\" equation expresses an important idea:\n",
    "- Any matrix $A$ with $rank(A) = r$ decomposes into a the sum of $r$ rank 1 matrices.\n",
    "- The significance of any term depends on its singular value $\\sigma_j$ (because $u_j$ and $v_j$ are unit vectors).\n",
    "- Due to the mutual orthogonality of the ${u_j}$ and ${v_j}$, there is a nested set of subspaces of matrices that can be formed from truncation of ${u_j}$ and ${v_j}$:\n",
    "    - $R_1 = span\\left(u_1\\right) \\cdot span\\left(v_1^T\\right)$\n",
    "    - $R_2 = span\\left(u_1, u_2\\right) \\cdot span\\left(v_1^T, v_2^T\\right)$\n",
    "    - $R_2 = span\\left(u_1, u_2\\right) \\cdot span\\left(v_1^T, v_2^T\\right)$\n",
    "    - $R_r = span\\left(u_1, u_2, \\ldots, u_r\\right) \\cdot span\\left(v_1^T, v_2^T, \\ldots, v_r^T\\right)$\n",
    "    - $R_1 \\sube R_2 \\sube \\ldots \\sube R_r$\n",
    "    - The best approximation of $A$ in $R_1$ is $\\sigma_1 u_1 v_1^T$.\n",
    "    - The best approximation of $A$ in $R_2$ is $\\sigma_1 u_1 v_1^T + \\sigma_2 u_2 v_2^T$.\n",
    "    - The best approximation of $A$ in $R_d$ is $\\sigma_1 u_1 v_1^T + \\sigma_2 u_2 v_2^T + \\ldots + \\sigma_d u_d v_d^T$\n",
    "    - The best approximation of $A$ in $R_r$ is $\\sigma_1 u_1 v_1^T + \\sigma_2 u_2 v_2^T + \\ldots + \\ldots + \\sigma_r u_r v_r^T$.\n",
    "    - These are results of the Eckert-Young Theorem that dates back to 1936!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The big idea (or should I say \"big data idea\") goes as follows:\n",
    "- The \"gist\" of a large matrix can be captured by a small number of well chosen vectors. \n",
    "- Determine a suitable subspace with a reasonable number of basis vectors, inspect the singular values. \n",
    "- Once the $\\sigma_j$ value is small, the approximation of $A$ improves only slightly in the larger subspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For those interested in data-driven analysis, numerous examples and applications are presented in Chapters 15-23 and 26; and Strang's book presents ample material on the mathematical, geometrical, and computational aspects of the SVD and related concepts. Also, you can check out the awesome videos on Prof. Brunton's YouTube channel..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the SVD\n",
    "\n",
    "As the SVD has become frequently used, a LOT of effort has gone into developing accurate and efficient methods for actually computing SVDs. See \"Computation of the Singular Value Decomposition\" by Cline and Dhillon (Ch. 58 of \"Handbook of Linear Algebra,\" 2nd Edition by Leslie Hogben) for details.\n",
    "\n",
    "SVD computation is sufficiently involved that it is advisable to use well-tested (and well-maintained) library functions to perform the SVD computation. In Python, the SVD is included in the linear algebra and scientific computation libraries:\n",
    "- `numpy.linalg.svd` computes the SVD.\n",
    "- `scipy.linalg.svd` also computes the SVD.\n",
    "- `scipy.linalg.svdvals` computes only the singular values and not the full factorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to consider an approach to computing the SVD for which you already understand the basics, try this version based on an idea due to Lanczos circa 1961. Instead of considering just the $m \\times n$ matrix $A$ or its transpose/adjoint, group them together a follows:\n",
    "\n",
    "$$ \\begin{bmatrix}\n",
    "0_{m \\times m} & A \\\\\n",
    "A^T & 0_{n \\times n}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvalues of this matrix turn out to be the singular values of $A$ (but also repreated with a negative sign). The associated eigenvectors include the right singular vectors (as the first $m$ entries) and the left singular vectors (as the final $n$ entries) - although some renormalization will be required.\n",
    "\n",
    "Below is a small example of executing the Lanczos approach where we form the matrix $M$, compute the eigenvalues and eigenvectors, identify the singular values and singular vectors, and show that the SVD provides the information necessary to reconstruct the matrix $A$ as a sum of rank 1 matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 2.],\n",
       "        [3., 4.],\n",
       "        [5., 6.]]),\n",
       " array([[1., 3., 5.],\n",
       "        [2., 4., 6.]]),\n",
       " array([[0., 0., 0., 1., 2.],\n",
       "        [0., 0., 0., 3., 4.],\n",
       "        [0., 0., 0., 5., 6.],\n",
       "        [1., 3., 5., 0., 0.],\n",
       "        [2., 4., 6., 0., 0.]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "\n",
    "\n",
    "# define a sample matrix A and form the M matrix as suggested by Lanczos\n",
    "A = 1.*np.array([[1,2],[3,4],[5,6]])\n",
    "m,n = A.shape\n",
    "M = np.block([[np.zeros([m,m]), A],[A.T, np.zeros([n,n])]])\n",
    "A, A.T,M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 9.53, -9.53,  0.51, -0.51, -0.  ]),\n",
       " array([[ 0.16,  0.16, -0.62,  0.62,  0.41],\n",
       "        [ 0.37,  0.37, -0.17,  0.17, -0.82],\n",
       "        [ 0.58,  0.58,  0.28, -0.28,  0.41],\n",
       "        [ 0.44, -0.44,  0.56,  0.56,  0.  ],\n",
       "        [ 0.56, -0.56, -0.44, -0.44, -0.  ]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the eigenvalues and eigenvectors of M using a python library function\n",
    "# Note that you already know a way to do this based on what you did in your homeworks:\n",
    "# Find leading eigenvalue/vector by matrix iteration\n",
    "# Construct projection matrix P to collapse along the direction of the lead eigenvector\n",
    "# Perfrom iteration with the matrix PA to find the second leading eigenvalue/vector\n",
    "vals, vecs = np.linalg.eig(M)\n",
    "vals, vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9.525518091565104, array([0.16, 0.37, 0.58, 0.44, 0.56]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign leading (largest positive) eigenvalue as lead singular value s0\n",
    "s0 = vals[0]\n",
    "# Assign associated eigenvector as evec0\n",
    "evec0 = vecs.T[0]\n",
    "s0,evec0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.16, 0.37, 0.58, 0.44, 0.56]), array([0.16, 0.37, 0.58, 0.44, 0.56]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that evec0 is the eigenvector of M with eigenvalue s0\n",
    "evec0, M@evec0/s0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.62, 0.78])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify right singular vector as last n entries in the eigenvector\n",
    "v0 = evec0[-2:]\n",
    "v0 = v0/np.linalg.norm(v0) #normalize the singular vector\n",
    "v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.23, 0.52, 0.82])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify left singular vector as first m entries in the eigenvector\n",
    "u0 = evec0[0:3]\n",
    "u0 = u0/np.linalg.norm(u0) #normalize the singular vector\n",
    "u0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.36, 1.72],\n",
       "        [3.1 , 3.92],\n",
       "        [4.84, 6.13]]),\n",
       " array([[1., 2.],\n",
       "        [3., 4.],\n",
       "        [5., 6.]]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute first approximation of A based on 1 left/right singular vector and display with A for comparison\n",
    "s0*np.outer(u0,v0), A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5143005806586438, array([-0.62, -0.17,  0.28,  0.56, -0.44]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Identify the next singular value s1 (as next largest positive eigenvector of M)\n",
    "s1 = vals[2]\n",
    "evec1 = vecs.T[2] #assign associated eigenvector to evec1\n",
    "s1,evec1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5143005806586438, array([-0.88, -0.24,  0.4 ]), array([ 0.78, -0.62]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign v1, u1 as first 3 and last 2 entries of evec1 and normalize\n",
    "v1 = evec1[-2:]\n",
    "v1 = v1/np.linalg.norm(v1)\n",
    "u1 = evec1[0:3]\n",
    "u1 = u1/np.linalg.norm(u1)\n",
    "s1, u1, v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [3., 4.],\n",
       "       [5., 6.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute best approximation of A based on 2 left/right singular vectors\n",
    "s0*np.outer(u0,v0) + s1*np.outer(u1,v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sum of 2 rank 1 matrices reconstructs A (to within roundoff error) consistent with the fact that $rank(A) = 2$. \n",
    "\n",
    "Much of the power of the SVD lies in the fact that, in many applications, matrices with large rank can be approximated quite well with a surprisingly small number of rank 1 terms based on the singular values and singular vectors."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "rise": {
   "enable_chalkboard": true,
   "footer": "<span style='color:blue '> <h2>ME535 Computational Techniques for Engineers, Winter 2021, University of Washington-Seattle, Prof. Duane Storti</h2> </span>",
   "height": "80%",
   "scroll": true,
   "slideNumber": true,
   "transition": "none",
   "width": "80%"
  },
  "vscode": {
   "interpreter": {
    "hash": "5955d872c5e3b115a70ba942f8258c852b234444ba9c9ee43b7b13e10670bb8e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
